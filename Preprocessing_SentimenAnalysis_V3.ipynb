{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Textual Data preproccessing**\n"
      ],
      "metadata": {
        "id": "8gIMH96b3Rxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Neccessary Libraries**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Sj3G5b_C3XxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK\n",
        "# Natural Language Toolkit (NLTK) library, a powerful tool for natural language processing tasks\n",
        "!pip install nltk\n",
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YuioFbRfqVp",
        "outputId": "09c3a472-e4d3-4608-89b7-2dbeabd1e9fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Neccessary Libraries**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "y8aoIVVNR5X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "# Import BeautifulSoup library to remove html tags\n",
        "import random\n",
        "\n",
        "# Import the regex module\n",
        "import re\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Import NLTK and download necessary resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the stopwords list\n",
        "nltk.download('stopwords')\n",
        "# Import the stopwords module after downloading it\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Import string library for removing punctuation marks and special characters\n",
        "import string\n",
        "\n",
        "# Import NLP Stemming and Lemmatization Libraries\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from transformers import BertTokenizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG7CHlzO3UMr",
        "outputId": "a83710f1-be62-4790-aed4-e6958b9b93fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount to Google Drive**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BPtAoZmuSW-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount (connect to) Google drive to be able to read from it (copy data files into HDFS)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRWDTu_q3gzE",
        "outputId": "ce33c29d-1f93-408a-80b0-9774fff5dee1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read json datasets from google drive\n"
      ],
      "metadata": {
        "id": "aQU4gYZw3rx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the  file path\n",
        "data_file_path = '/content/drive/My Drive/cryptodata/bitcointalk.json'\n",
        "\n",
        "# Read the  dataset\n",
        "with open(data_file_path , 'r') as file:\n",
        "    dataset_bitcointalk = json.load(file)"
      ],
      "metadata": {
        "id": "6kUHmUK33snQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_bitcointalk[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PFOFykm38JR",
        "outputId": "b0508ff7-15a2-4c38-832b-d3265047493b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'thread_id': 0, 'date': 1498867765000, 'text': 'if you wanna have a better security, i would recommend to have a multi signature wallet (the address with prefix of 3 instead of 1). in case of possibility of being hacked, they need certainly large amount of computing power which consumes all the possible energy in the Earth as there are more than googol (IIRC) of possible combinations and not all the private key is valid for bitcoin address.', 'post_id': 0}, {'thread_id': 0, 'date': 1498868180000, 'text': \"Quote from: ayurvedicurea2growtaller on July 01, 2017, 12:04:49 AMcan they hack into our wallet. There are many professional hackers out there. What are the chances? Also what is the safest way to safeguard ourself?It is possible, 100%. That is if you don't have sufficient security measures. If you adopt good security measures, the chances are next to zero. Bitcoin don't have any vulnerability that allows anyone to guess your private key with a fair amount of computing power.If you want to safeguard yourself, you have to create a cold storage. Multisig won't work at all if all the addresses are generated on the same computer.\", 'post_id': 1}, {'thread_id': 0, 'date': 1498869531000, 'text': 'Quote from: lottery248 on July 01, 2017, 12:09:25 AMif you wanna have a better security, i would recommend to have a multi signature wallet (the address with prefix of 3 instead of 1). in case of possibility of being hacked, they need certainly large amount of computing power which consumes all the possible energy in the Earth as there are more than googol (IIRC) of possible combinations and not all the private key is valid for bitcoin address.Seems like you are theorizing about brute force hack, in which case multi-signature doesn\\'t really matter, because even a single private key can\\'t be hacked in that way. But it\\'s possible that your bitcoin wallet get \"stolen\" by a specific virus, and your password too with a keylogger, so hackers can get access to your wallet without cracking private keys. There are also viruses that replace Bitcoin addresses on clipboard when you are just about to send coins. So, keeping your PC safe is #1 priority when you are using Bitcoin.', 'post_id': 2}, {'thread_id': 0, 'date': 1498869635000, 'text': \"Quote from: lottery248 on July 01, 2017, 12:09:25 AMif you wanna have a better security, i would recommend to have a multi signature wallet (the address with prefix of 3 instead of 1). in case of possibility of being hacked, they need certainly large amount of computing power which consumes all the possible energy in the Earth as there are more than googol (IIRC) of possible combinations and not all the private key is valid for bitcoin address.You're the one whose liable of your private key and for me the safest way to prevent those hackers to enter your privacy is turn off your location. More of those attackers were now traveling around the internet network searching for good stuff. Avoid sharing your personal informations like e-mails with those strangers, you might be linked with that which causes you to be more vurnerable of hacks. Strong wallet security keys should be combined and don't let it be transparent to anybody even your closest friend might be the sources of private informations to leak.\", 'post_id': 3}, {'thread_id': 0, 'date': 1498872766000, 'text': \"No, thats the good thing about Bitcoin, is that you have security and privacy.  Of course if you have an online wallet, your password may be stolen somehow, especially if you use same passwords in many sites. Just keep your private keys safe, and don't let anyone have access to it. \", 'post_id': 4}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to DataFrame**"
      ],
      "metadata": {
        "id": "7c3xdZktQpFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset to DataFrame\n",
        "df = pd.DataFrame(dataset_bitcointalk)[['date', 'text']]"
      ],
      "metadata": {
        "id": "bqL0cUa7QdLu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAxA-cMgQidg",
        "outputId": "4453cb26-0262-4726-f30f-8b2f8009caa7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            date                                               text\n",
            "0  1498867765000  if you wanna have a better security, i would r...\n",
            "1  1498868180000  Quote from: ayurvedicurea2growtaller on July 0...\n",
            "2  1498869531000  Quote from: lottery248 on July 01, 2017, 12:09...\n",
            "3  1498869635000  Quote from: lottery248 on July 01, 2017, 12:09...\n",
            "4  1498872766000  No, thats the good thing about Bitcoin, is tha...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert Date**"
      ],
      "metadata": {
        "id": "O_CsoiBqQ3sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert epoch timestamp to a date format\n",
        "df['date'] = pd.to_datetime(df['date'], unit='ms')"
      ],
      "metadata": {
        "id": "n2hfoI_CQxbC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNezl2j8Q0X9",
        "outputId": "1b2d7f11-5b75-46ea-c563-d21de29451bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 date                                               text\n",
            "0 2017-07-01 00:09:25  if you wanna have a better security, i would r...\n",
            "1 2017-07-01 00:16:20  Quote from: ayurvedicurea2growtaller on July 0...\n",
            "2 2017-07-01 00:38:51  Quote from: lottery248 on July 01, 2017, 12:09...\n",
            "3 2017-07-01 00:40:35  Quote from: lottery248 on July 01, 2017, 12:09...\n",
            "4 2017-07-01 01:32:46  No, thats the good thing about Bitcoin, is tha...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Sampling**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7seeYMA86TG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of elements to sample (10% of the dataset)\n",
        "sample_size = int(len(df) * 0.1)\n",
        "\n",
        "# Perform random sampling\n",
        "random_sample = df.sample(n=sample_size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k1mRXott6SxZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_sample[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK8jxUyy3uVP",
        "outputId": "89480d34-133a-4500-b00e-7c1efb5668f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       date                                               text\n",
            "1019776 2018-07-14 06:57:24  Quote from: Bytem3 on July 13, 2018, 08:42:31 ...\n",
            "1028637 2018-07-23 20:31:20  Quote from: OSEIBOATENG on July 07, 2018, 12:5...\n",
            "326890  2016-02-25 23:36:04  Quote from: QuestionAuthority on February 25, ...\n",
            "494656  2018-03-06 06:29:34  Quote from: CryptoJoop on March 03, 2018, 04:1...\n",
            "592901  2018-01-11 02:13:43  try ConnectJob ICO bounty, join only you like ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, \"random_sample\" contains 10% of the data from \"dataset_bitcointalk\".\n"
      ],
      "metadata": {
        "id": "G-5CIjyk7aYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lowercasing**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cHsm0rwmqyDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase the 'text' column\n",
        "random_sample['text'] = random_sample['text'].str.lower()\n",
        "\n",
        "# Print the first few rows to verify\n"
      ],
      "metadata": {
        "id": "gBzXrUnXSipw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing Special Characters and Punctuation**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Remove any unnecessary special characters and punctuation marks that may not contribute to the sentiment analysis\n"
      ],
      "metadata": {
        "id": "BEP2lzPK1kq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove punctuation from the 'text' column\n",
        "random_sample['text'] = random_sample['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "\n",
        "# Print the first few rows to verify\n",
        "print(random_sample.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJALN80ST-R8",
        "outputId": "324c47db-dd82-4d1a-e722-1c1a52d32218"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       date                                               text\n",
            "1019776 2018-07-14 06:57:24  quote from bytem3 on july 13 2018 084231 pmacc...\n",
            "1028637 2018-07-23 20:31:20  quote from oseiboateng on july 07 2018 125602 ...\n",
            "326890  2016-02-25 23:36:04  quote from questionauthority on february 25 20...\n",
            "494656  2018-03-06 06:29:34  quote from cryptojoop on march 03 2018 041947 ...\n",
            "592901  2018-01-11 02:13:43  try connectjob ico bounty join only you like t...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see now there is not special characters or punctuation in the dataset."
      ],
      "metadata": {
        "id": "G6M84jWv3aiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove URLs**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this step, we will determine whether there are any URLs present. If URLs are found, we will proceed to remove them; otherwise, we will move on to the next steps."
      ],
      "metadata": {
        "id": "JaDjmcITnSsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check URLs Presece"
      ],
      "metadata": {
        "id": "P1_NpgzwnWnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for URLs\n",
        "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "contains_urls = random_sample['text'].str.contains(url_pattern, regex=True)\n",
        "\n",
        "# Check if any row contains a URL and print the corresponding message\n",
        "if contains_urls.any():\n",
        "    print(\"There is a URL in the text.\")\n",
        "else:\n",
        "    print(\"There are no URLs in the text.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_blf1GhgnSaZ",
        "outputId": "f354dfac-ff43-4d01-9e5d-5b5633bba824"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are no URLs in the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As evident from the dataset, there are no URLs present.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jjb6SBONr2sZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove Numbers**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AmuiQHJ1eq3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the presence of the numbers"
      ],
      "metadata": {
        "id": "uZXeOZT8wY3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Numbers\n",
        "number_pattern = r'\\d+'  # This pattern will match one or more digits\n",
        "contains_numbers = random_sample['text'].str.contains(number_pattern, regex=True)\n",
        "\n",
        "# Check if any row contains a number and print the corresponding message\n",
        "if contains_numbers.any():\n",
        "    print(\"There are numbers in the text.\")\n",
        "else:\n",
        "    print(\"There are no numbers in the text.\")\n"
      ],
      "metadata": {
        "id": "JV2Q0BAMewIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e728542-fff8-4ab5-8e13-d8cd1e7eb560"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are numbers in the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Numbers"
      ],
      "metadata": {
        "id": "6EFHJjIWxjWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers from the 'text' column\n",
        "random_sample['text'] = random_sample['text'].str.replace(r'\\d+', '', regex=True)\n",
        "\n",
        "# Print the first few rows to verify\n",
        "print(random_sample.head())\n"
      ],
      "metadata": {
        "id": "nCeNuxvQe2aE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f7aca6-77ab-40a5-892c-77ddba4b5631"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       date                                               text\n",
            "1019776 2018-07-14 06:57:24  quote from bytem on july    pmaccording to jam...\n",
            "1028637 2018-07-23 20:31:20  quote from oseiboateng on july    pmi have bee...\n",
            "326890  2016-02-25 23:36:04  quote from questionauthority on february    pm...\n",
            "494656  2018-03-06 06:29:34  quote from cryptojoop on march    pma nice way...\n",
            "592901  2018-01-11 02:13:43  try connectjob ico bounty join only you like t...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the presence of the numbers after remove"
      ],
      "metadata": {
        "id": "LtX4cTBvxWgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Numbers\n",
        "number_pattern = r'\\d+'  # This pattern will match one or more digits\n",
        "contains_numbers = random_sample['text'].str.contains(number_pattern, regex=True)\n",
        "\n",
        "# Check if any row contains a number and print the corresponding message\n",
        "if contains_numbers.any():\n",
        "    print(\"There are numbers in the text.\")\n",
        "else:\n",
        "    print(\"There are no numbers in the text.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHvcF9gKxOWf",
        "outputId": "c0d2ec25-851b-4fc6-c2be-25298d30f449"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are no numbers in the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Tokenization & Numerical Representation for BERT**\n",
        "\n",
        "\n",
        "---\n",
        "In this step, we focus on the preparation of textual data to be used with the BERT (Bidirectional Encoder Representations from Transformers) model. This process involves two main components: tokenization and numerical representation. This step is crucial for BERT-based models because they require a specific input format to produce meaningful contextual embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "SUbD8YmBPaDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "KVvSV9uLxQx3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the 'text' column directly\n",
        "random_sample['text'] = random_sample['text'].apply(lambda x: tokenizer.tokenize(x))\n",
        "\n",
        "# Rename the 'text' column to 'tokens'\n",
        "random_sample.rename(columns={'text': 'tokens'}, inplace=True)\n",
        "\n",
        "# Print the first few rows to verify\n",
        "print(random_sample.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTrOMuksxSmQ",
        "outputId": "c4bd02c0-3762-4f3a-93f3-5e9c7d13e350"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       date                                             tokens\n",
            "1019776 2018-07-14 06:57:24  [quote, from, byte, ##m, on, july, pm, ##ac, #...\n",
            "1028637 2018-07-23 20:31:20  [quote, from, os, ##ei, ##boat, ##eng, on, jul...\n",
            "326890  2016-02-25 23:36:04  [quote, from, question, ##au, ##thor, ##ity, o...\n",
            "494656  2018-03-06 06:29:34  [quote, from, crypt, ##oj, ##oop, on, march, p...\n",
            "592901  2018-01-11 02:13:43  [try, connect, ##jo, ##b, ic, ##o, bounty, joi...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove Stopwords**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qGaRe3qD8eDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count the number of stopwords"
      ],
      "metadata": {
        "id": "4rsQYlIg-zAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Count the total number of stopwords in the entire 'tokens' column using a generator expression\n",
        "total_stopwords = sum(sum(1 for word in tokens if word in stop_words) for tokens in random_sample['tokens'])\n",
        "\n",
        "# Print the total number of stopwords\n",
        "print(f\"Total number of stopwords in the sample: {total_stopwords}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEBP9L0b4OIm",
        "outputId": "db24d211-f889-4dc9-9233-ab706ab165b7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of stopwords in the sample: 7243030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our dataset, there are 7243030 stopwords that need to be removed."
      ],
      "metadata": {
        "id": "bTT-6LsV_eDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove The Stop Words"
      ],
      "metadata": {
        "id": "2Tk70LAoEMMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords from each tokenized text\n",
        "random_sample['tokens'] = random_sample['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
        "# Rename the 'tokens' column to 'tokens'\n",
        "random_sample.rename(columns={'tokens': 'clean_tokens'}, inplace=True)\n"
      ],
      "metadata": {
        "id": "NWLrvZ8V3sf0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Number of Stopwords After Remove\n"
      ],
      "metadata": {
        "id": "kUo12Mg4EXMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Count the total number of stopwords in the entire 'tokens' column using a generator expression\n",
        "total_stopwords = sum(sum(1 for word in tokens if word in stop_words) for tokens in random_sample['clean_tokens'])\n",
        "\n",
        "# Print the total number of stopwords\n",
        "print(f\"Total number of stopwords in the sample: {total_stopwords}\")\n"
      ],
      "metadata": {
        "id": "YRsCwVTT93G-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0aaebcf-0d9c-401a-837c-066fac742ce9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of stopwords in the sample: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it appears that there are no stop words present in our dataset."
      ],
      "metadata": {
        "id": "wn1a7oWM_w3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **stemming or lemmatization**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<div style=\"text-align: justify\">\n",
        "In this phase of the process, we will delve into the realm of stemming and lemmatization. Stemming and lemmatization are fundamental techniques in the realm of Natural Language Processing (NLP). Their primary goal is to streamline words into their core or root forms, a transformation that plays a pivotal role in enhancing both the effectiveness and precision of subsequent stages.\n",
        "</div>"
      ],
      "metadata": {
        "id": "X9GbeuUUBw4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "8tTv81dwbVQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem each word in the 'clean_tokens' column\n",
        "random_sample['clean_tokens'] = random_sample['clean_tokens'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n"
      ],
      "metadata": {
        "id": "UZPNoPc2QDUj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "TpEnIgqzcxgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize each word in the 'clean_tokens' column\n",
        "random_sample['clean_tokens'] = random_sample['clean_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n"
      ],
      "metadata": {
        "id": "mC2G9QcBB9YH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename the Column"
      ],
      "metadata": {
        "id": "GF9Q66KN8cki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the 'clean_tokens' column to a desired name, for instance 'processed_tokens'\n",
        "random_sample.rename(columns={'clean_tokens': 'processed_tokens'}, inplace=True)\n"
      ],
      "metadata": {
        "id": "9Xi0xQEz8YPg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few rows to verify\n",
        "print(random_sample.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCN9XfGS9LRO",
        "outputId": "1c465951-ace3-43eb-d7ab-58b0d755bbfb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       date                                   processed_tokens\n",
            "1019776 2018-07-14 06:57:24  [quot, byte, ##m, juli, pm, ##ac, ##cor, ##din...\n",
            "1028637 2018-07-23 20:31:20  [quot, o, ##ei, ##boat, ##eng, juli, pm, ##i, ...\n",
            "326890  2016-02-25 23:36:04  [quot, question, ##au, ##thor, ##iti, februari...\n",
            "494656  2018-03-06 06:29:34  [quot, crypt, ##oj, ##oop, march, pm, ##a, nic...\n",
            "592901  2018-01-11 02:13:43  [tri, connect, ##jo, ##b, ic, ##o, bounti, joi...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save the Prepared Dataset**\n"
      ],
      "metadata": {
        "id": "kJ1t8TtbQp9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path where you want to save the file on your Google Drive\n",
        "path_to_save = \"/content/drive/My Drive/Dissertation/processed_bitcointalk.csv\"\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "random_sample.to_csv(path_to_save, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkXYDbC4PUL7"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}