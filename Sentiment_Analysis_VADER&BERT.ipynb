{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nTeVWoMrL4m"
      },
      "source": [
        "# **Sentiment Analysis Using BERT & VEDAR**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFXBn8USrU9O"
      },
      "source": [
        "In this notebook, we explore sentiment analysis with a special focus on the 'Bitcointalk' dataset. Our objective is to extract sentiment scores utilising two powerful techniques:\n",
        "* BERT (Bidirectional Encoder Representations from Transformers), which represents a significant breakthrough in transformer-driven machine learning.\n",
        "* VADER (Valence Aware Dictionary and sEntiment Reasoner), an instrument for lexicon-based sentiment analysis designed for social media expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12XUJVe9hyVB"
      },
      "source": [
        "# **Install Required Libraries**\n",
        "---\n",
        "Firstly, we install and import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uYkC7Sbuhyt5"
      },
      "outputs": [],
      "source": [
        "# Install transformers library from Hugging Face\n",
        "!pip install transformers\n",
        "\n",
        "# Install the vaderSentiment library, for ectracting scores using VADER\n",
        "!pip install vaderSentiment\n",
        "\n",
        "# Install tqdm for displaying progress bars in loops and during data download/upload\n",
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLynfEydh_tZ"
      },
      "source": [
        "# **Import Required Libraries**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sYOV75pciDvS"
      },
      "outputs": [],
      "source": [
        "# Importing the pandas library for data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the NumPy library\n",
        "import numpy as np\n",
        "\n",
        "# Importing the json library for JSON file operations\n",
        "import json\n",
        "\n",
        "# Importing the torch library, the main PyTorch module\n",
        "import torch\n",
        "\n",
        "# Importing the AutoTokenizer and AutoModelForSequenceClassification modules from transformers\n",
        "# for tokenization and sequence classification tasks\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Importing the SentimentIntensityAnalyzer module from vaderSentiment for sentiment analysis tasks\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Importing the tqdm library to display progress bars in loops\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz_sls1grbB7"
      },
      "outputs": [],
      "source": [
        "# Mount (connect to) Google drive to be able to read from it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo8N09aorT5s"
      },
      "outputs": [],
      "source": [
        "# Define the  file path\n",
        "data_file_path = '/content/drive/My Drive/MyData/bitcointalk.json'\n",
        "\n",
        "# Read the  dataset\n",
        "with open(data_file_path , 'r') as file:\n",
        "    dataset_bitcointalk = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpYiEJSMnd2G"
      },
      "source": [
        "# **EDA & Data Cleaning**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUTLGx7On0Oa"
      },
      "source": [
        "First of all, we get an overview of the first five variables of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQiTk5Ubmo3K"
      },
      "outputs": [],
      "source": [
        "print(dataset_bitcointalk[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwkymhtuy_EG"
      },
      "source": [
        "Based on the basic overview of the data, the dataset columns are:\n",
        "\n",
        "thread_id: Unique identifier for discussion threads.\n",
        "date: Timestamp of the post, in Unix format.\n",
        "text: Content of the post.\n",
        "post_id: Unique identifier for each post."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rs78WwMpuXH"
      },
      "outputs": [],
      "source": [
        "print(type(dataset_bitcointalk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_qPQD1zwA6s"
      },
      "source": [
        "Upon examination, we observe that the dataset is in a list format. It is crucial to transform this into a pandas dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsvQ3RmVwJe7"
      },
      "source": [
        "## **Select Relevant Columns & Convert to Pandas Dataframe**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For the objectives of our analysis, only the 'date' and 'text' columns are essential. Therefore, we will retain just these two columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE66eoOAsFgj"
      },
      "outputs": [],
      "source": [
        "# Extracting only the 'date' and 'text' columns\n",
        "df_bitcointalk = pd.DataFrame(dataset_bitcointalk)[['date', 'text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbdf_YIhkTKD"
      },
      "outputs": [],
      "source": [
        "# Display the first 5 rows of the data\n",
        "print(df_bitcointalk[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRc4bO30lsxO"
      },
      "source": [
        "# **Random Sampling**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Given our computational constraints, we will focus on a random subset of the data. To achieve this, we will employ random sampling techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjWUHTQ1lyCV"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of elements to sample (5% of the dataset)\n",
        "sample_size = int(len(df_bitcointalk) * 0.05)\n",
        "\n",
        "# Perform random sampling\n",
        "df_text = df_bitcointalk.sample(n=sample_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA5t1AzgsdoG"
      },
      "outputs": [],
      "source": [
        "# Display the first 5 rows of the df_text dataframe for a quick overview\n",
        "df_text.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaDjmcITnSsy"
      },
      "source": [
        "# **Handle URLs**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this step, we will determine whether there are any URLs present. If URLs are found, we will proceed to remove them; otherwise, we will move on to the next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1_NpgzwnWnn"
      },
      "source": [
        "## Check URLs Presence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_blf1GhgnSaZ"
      },
      "outputs": [],
      "source": [
        "# Check for URLs\n",
        "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "contains_urls = df_text['text'].str.contains(url_pattern, regex=True)\n",
        "\n",
        "# Check if any row contains a URL and print the corresponding message\n",
        "if contains_urls.any():\n",
        "    print(\"There is a URL in the text.\")\n",
        "else:\n",
        "    print(\"There are no URLs in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjb6SBONr2sZ"
      },
      "source": [
        "As evident from the dataset, there are URLs present. so we will remove them using \"str.replace()\" method.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVBx5876keak"
      },
      "source": [
        "## Remove URLs\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NdyIwKjkLiy"
      },
      "outputs": [],
      "source": [
        "# Remove URLs\n",
        "df_text['text'] = df_text['text'].str.replace(url_pattern, '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SDKWFjPknd5"
      },
      "source": [
        "## Verify After Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hUILtMbkz1y"
      },
      "outputs": [],
      "source": [
        "# Check for URLs\n",
        "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "contains_urls = df_text['text'].str.contains(url_pattern, regex=True)\n",
        "\n",
        "# Check if any row contains a URL and print the corresponding message\n",
        "if contains_urls.any():\n",
        "    print(\"There is a URL in the text.\")\n",
        "else:\n",
        "    print(\"There are no URLs in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmuiQHJ1eq3g"
      },
      "source": [
        "# **Handle Numbers**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZXeOZT8wY3Q"
      },
      "source": [
        "## Check the Presence of the Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV2Q0BAMewIz"
      },
      "outputs": [],
      "source": [
        "# Check for Numbers\n",
        "number_pattern = r'\\d+'  # This pattern will match one or more digits\n",
        "contains_numbers = df_text['text'].str.contains(number_pattern, regex=True)\n",
        "\n",
        "# Check if any row contains a number and print the corresponding message\n",
        "if contains_numbers.any():\n",
        "    print(\"There are numbers in the text.\")\n",
        "else:\n",
        "    print(\"There are no numbers in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EFHJjIWxjWy"
      },
      "source": [
        "## Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCeNuxvQe2aE"
      },
      "outputs": [],
      "source": [
        "# Remove numbers from the 'text' column\n",
        "df_text['text'] = df_text['text'].str.replace(r'\\d+', '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtX4cTBvxWgQ"
      },
      "source": [
        "## Verify After Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHvcF9gKxOWf"
      },
      "outputs": [],
      "source": [
        "# Check for Numbers\n",
        "number_pattern = r'\\d+'  # This pattern will match one or more digits\n",
        "contains_numbers = df_text['text'].str.contains(number_pattern, regex=True)\n",
        "\n",
        "# Check if any row contains a number and print the corresponding message\n",
        "if contains_numbers.any():\n",
        "    print(\"There are numbers in the text.\")\n",
        "else:\n",
        "    print(\"There are no numbers in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjDYML-sk45F"
      },
      "source": [
        "We have successfully removed all numbers from our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPL8_amI1ExD"
      },
      "source": [
        "# **Handle Mentions**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUbqTDVW1Lk6"
      },
      "source": [
        "## Check Mentions Presence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RBi_o9o1JsI"
      },
      "outputs": [],
      "source": [
        "# Define pattern for mentions\n",
        "mention_pattern = r'@\\w+'\n",
        "contains_mentions = df_text['text'].str.contains(mention_pattern, regex=True)\n",
        "\n",
        "# Check if any row contains a mention and print the corresponding message\n",
        "if contains_mentions.any():\n",
        "    print(\"There are mentions in the text.\")\n",
        "else:\n",
        "    print(\"There are no mentions in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiFXqepv1TJF"
      },
      "source": [
        "## Remove Mentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrWBMDoF1gK3"
      },
      "outputs": [],
      "source": [
        "# Remove mentions from the 'text' column\n",
        "df_text['text'] = df_text['text'].str.replace(mention_pattern, '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COWkLZG51oYQ"
      },
      "source": [
        "## Verify After Remove Mentions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwXCbckJ1zvY"
      },
      "outputs": [],
      "source": [
        "contains_mentions_after = df_text['text'].str.contains(mention_pattern, regex=True)\n",
        "if contains_mentions_after.any():\n",
        "    print(\"There are mentions in the text after removal.\")\n",
        "else:\n",
        "    print(\"There are no mentions in the text after removal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwsentUNR_Yw"
      },
      "source": [
        "# **Transform Epoch to Date Format**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L706wK4UsnOg"
      },
      "outputs": [],
      "source": [
        "#Convert epoch timestamp to a date format\n",
        "df_text['date'] = pd.to_datetime(df_text['date'], unit='ms')\n",
        "\n",
        "print(df_text[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbR4xSf-BtVl"
      },
      "source": [
        "Now the dataset is ready to extract sentiment scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysy0yWCXEuu-"
      },
      "source": [
        "# **Extract Sentiment Scores Using VADER**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp5RSz3VE3iB"
      },
      "outputs": [],
      "source": [
        "# Initialize the VADER sentiment intensity analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Apply the analyzer to each token in the processed_tokens column\n",
        "df_text['VADER_scores'] = df_text['text'].apply(lambda x: analyzer.polarity_scores(x))\n",
        "df_text['compound'] = df_text['VADER_scores'].apply(lambda d: d['compound'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLv5xPdRJJfG"
      },
      "outputs": [],
      "source": [
        "# Print the first few rows with the VADER scores\n",
        "print(df_text.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf3tnlW4xAga"
      },
      "source": [
        "# **Categorizing Sentiments (VADER)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This section of the code we use a function to classify sentiment based on the compound score obtained from the VADER sentiment analysis tool.\n",
        "\n",
        "In this approach:\n",
        "* A sentiment is considered positive if its compound score is greater than 0.05.\n",
        "* It's considered negative if the score is less than -0.05.\n",
        "* All other scores fall into the neutral category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSwwb3IeGklZ"
      },
      "outputs": [],
      "source": [
        "def categorize_sentiment(compound_score):\n",
        "    if compound_score > 0.05:\n",
        "        return 1\n",
        "    elif compound_score < -0.05:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df_text['VADER_Sentiment_Scores'] = df_text['compound'].apply(categorize_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK6UA_9RG3Ff"
      },
      "outputs": [],
      "source": [
        "# Have an overview of the first few rows of the data\n",
        "df_text.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFC_k1dFaTX5"
      },
      "source": [
        "# **Sentiment Analysis Using BERT**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMUe3Gtkae8I"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNiaVkK9bNTh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "def sentim_analyzer(df, tokenizer, model):\n",
        "\n",
        "    for i in tqdm(df.index):\n",
        "        try:\n",
        "            text_content = df.loc[i, 'text']\n",
        "        except:\n",
        "            return print(' \\'text\\' column might be missing from dataframe')\n",
        "\n",
        "        # Pre-process input\n",
        "        input = tokenizer(text_content, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "        # Estimate output\n",
        "        output = model(**input)\n",
        "\n",
        "        # Pass model output logits through a softmax layer.\n",
        "        predictions = softmax(output.logits, dim=-1)\n",
        "        df.loc[i, 'Positive'] = predictions[0][0].tolist()\n",
        "        df.loc[i, 'Negative'] = predictions[0][1].tolist()\n",
        "        df.loc[i, 'Neutral']  = predictions[0][2].tolist()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Use the modified function:\n",
        "df_text = sentim_analyzer(df_text, tokenizer, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1igx7bFnXQn"
      },
      "outputs": [],
      "source": [
        "df_text.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwdoYYxE1Kf3"
      },
      "source": [
        "# **Calculate Compound BERT**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slnRbAPn1PRL"
      },
      "outputs": [],
      "source": [
        "# Compute the intermediate compound score\n",
        "df_text['BERT_Compound_intermediate'] = df_text['Positive'] - df_text['Negative']\n",
        "\n",
        "# Normalize the score to be between -1 and 1 using tanh\n",
        "df_text['BERT_Compound'] = np.tanh(df_text['BERT_Compound_intermediate'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B-U-qW71yi2"
      },
      "outputs": [],
      "source": [
        "df_text.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHdm5spgkP1Q"
      },
      "source": [
        "# **Categorizing Sentiments (BERT)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq-W24dUkN9Y"
      },
      "outputs": [],
      "source": [
        "def categorize_sentiment(compound_score):\n",
        "    if compound_score > 0.05:\n",
        "        return 1\n",
        "    elif compound_score < -0.05:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df_text['BERT_Sentiment_Scores'] = df_text['BERT_Compound'].apply(categorize_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjxHQPdSjy5i"
      },
      "outputs": [],
      "source": [
        "df_text.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0EgoP7Xkl92"
      },
      "source": [
        "# **Drop Irrelevant Columns**\n",
        "\n",
        "---\n",
        "\n",
        "To drop irrelevant columns in a the 'df_text' DataFrame using pandas, we use the 'drop' method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfIKf_OAkqn_"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = [\"text\", \"VADER_scores\", \"Positive\", \"Negative\", \"Neutral\", \"BERT_Compound_intermediate\"]\n",
        "df_text = df_text.drop(columns=columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vZ9U3qllXw0"
      },
      "outputs": [],
      "source": [
        "df_text.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rR5bCu6n150"
      },
      "source": [
        "# **Save the Dataset**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this section, we'll save our dataset, which now includes sentiment scores derived from both VADER and BERT, back to Google Drive. This enriched dataset will later serve as a foundation for building deep learning models that leverage these sentiment scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayouiNVBn1dy"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/My Drive/MyData/Sentiment_Scores_Dataset.csv\"\n",
        "df_text.to_csv(path, index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}